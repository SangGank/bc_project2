{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing_dataset(dataset):\n",
    "  \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
    "  subject_entity = []\n",
    "  object_entity = []\n",
    "  subject_type= []\n",
    "  object_type= []\n",
    "  subject_start= []\n",
    "  object_start= []\n",
    "  subject_end= []\n",
    "  object_end= []\n",
    "  \n",
    "\n",
    "  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    # i = i[1:-1].split(',')[0].split(':')[1]\n",
    "    sub_entity = eval(i)\n",
    "    ob_entity = eval(j)\n",
    "    sub_word= sub_entity['word']\n",
    "    sub_type = sub_entity['type']\n",
    "    # j = j[1:-1].split(',')[0].split(':')[1]\n",
    "    ob_word = ob_entity['word']\n",
    "    ob_type = ob_entity['type']\n",
    "\n",
    "    ob_start = ob_entity['start_idx']\n",
    "    sub_start = sub_entity['start_idx']\n",
    "\n",
    "    ob_end = ob_entity['end_idx']\n",
    "    sub_end = sub_entity['end_idx']\n",
    "\n",
    "\n",
    "    subject_entity.append(sub_word)\n",
    "    object_entity.append(ob_word)\n",
    "    subject_type.append(sub_type)\n",
    "    object_type.append(ob_type)\n",
    "    subject_start.append(sub_start)\n",
    "    object_start.append(ob_start)\n",
    "    subject_end.append(sub_end)\n",
    "    object_end.append(ob_end)\n",
    "  \n",
    "  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],\n",
    "                              'subject_type' : subject_type, 'object_type': object_type,'sub_start' : subject_start,'ob_start':object_start,'sub_end' :subject_end ,'ob_end': object_end})\n",
    "  out_dataset.id = out_dataset.index\n",
    "  return out_dataset\n",
    "\n",
    "\n",
    "  \n",
    "def load_data(dataset_dir):\n",
    "  \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
    "  pd_dataset = pd.read_csv(dataset_dir)\n",
    "  dataset = preprocessing_dataset(pd_dataset)\n",
    "  \n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 기본 베이스 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에스타니슬라오 피게라스 [SEP] 스페인', '고등 연구 계획국 [SEP] 미국 국방부', '울버햄튼 [SEP] 잉글랜드', 'K3리그 베이직 [SEP] K3리그 어드밴스', '로날트 더 부르 [SEP] 네덜란드']\n",
      "0    1873년 스페인의 왕정이 폐지되고 공화국이 설립되자, 아마데오 1세는 퇴위당하고 ...\n",
      "1    인터넷은 1969 년 미국 국방부 부하의 고등 연구 계획국 (ARPA)에 의해 제안...\n",
      "2    손흥민의 토트넘은 15일 오후 11시 영국 울버햄튼의 몰리뉴 스타디움에서 울버햄튼 ...\n",
      "3    더불어 2017 시즌을 기점으로 K3리그가 K3리그 어드밴스로 개편되고, 하위 리그...\n",
      "4    또한 프랑크 더 부르의 쌍둥이 동생인 로날트 더 부르도 네덜란드 축구 국가대표팀과 ...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = e01 + ' [SEP] ' + e02\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_totalX3.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 단어를 토큰으로 바꾸기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에스타니슬라오 피게라스[SEP]스페인', '고등 연구 계획국[SEP]미국 국방부', '울버햄튼[SEP]잉글랜드', 'K3리그 베이직[SEP]K3리그 어드밴스', '로날트 더 부르[SEP]네덜란드']\n",
      "0    1873년 <LOC>의 왕정이 폐지되고 공화국이 설립되자, 아마데오 1세는 퇴위당하...\n",
      "1    인터넷은 1969 년 <ORG> 부하의 <ORG> (ARPA)에 의해 제안된 BBN...\n",
      "2    손흥민의 토트넘은 15일 오후 11시 영국 <ORG>의 몰리뉴 스타디움에서 <ORG...\n",
      "3    더불어 2017 시즌을 기점으로 K3리그가 <ORG>로 개편되고, 하위 리그로서 <...\n",
      "4    또한 프랑크 더 부르의 쌍둥이 동생인 <PER>도 <LOC> 축구 국가대표팀과 유럽...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_word(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f'<{sub_type}>',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f'<{ob_type}>',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = e01 + ' [SEP] ' + e02\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "  \n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "                                  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_totalX3.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 인덱스 스타트 번호 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에스타니슬라오 피게라스 [SEP] 46 [SEP] 스페인 [SEP] 6', '고등 연구 계획국 [SEP] 23 [SEP] 미국 국방부 [SEP] 12', '울버햄튼 [SEP] 24 [SEP] 잉글랜드 [SEP] 60', 'K3리그 베이직 [SEP] 49 [SEP] K3리그 어드밴스 [SEP] 24', '로날트 더 부르 [SEP] 21 [SEP] 네덜란드 [SEP] 31']\n",
      "0    1873년 스페인의 왕정이 폐지되고 공화국이 설립되자, 아마데오 1세는 퇴위당하고 ...\n",
      "1    인터넷은 1969 년 미국 국방부 부하의 고등 연구 계획국 (ARPA)에 의해 제안...\n",
      "2    손흥민의 토트넘은 15일 오후 11시 영국 울버햄튼의 몰리뉴 스타디움에서 울버햄튼 ...\n",
      "3    더불어 2017 시즌을 기점으로 K3리그가 K3리그 어드밴스로 개편되고, 하위 리그...\n",
      "4    또한 프랑크 더 부르의 쌍둥이 동생인 로날트 더 부르도 네덜란드 축구 국가대표팀과 ...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_word(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f'<{sub_type}>',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f'<{ob_type}>',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02, s01, s02 in zip(dataset['subject_entity'], dataset['object_entity'], dataset['sub_start'], dataset['ob_start']):\n",
    "    temp = ''\n",
    "    temp = e01 + ' [SEP] '+ f'{s01}' + ' [SEP] ' + e02+ ' [SEP] '+ f'{s02}'\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #                                                                 dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "  \n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "                                  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_totalX3.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 스타트 번호, 끝번호 둘다 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에스타니슬라오 피게라스 [SEP] 46 57 [SEP] 스페인 [SEP] 6 8', '고등 연구 계획국 [SEP] 23 31 [SEP] 미국 국방부 [SEP] 12 17', '울버햄튼 [SEP] 24 27 [SEP] 잉글랜드 [SEP] 60 63', 'K3리그 베이직 [SEP] 49 56 [SEP] K3리그 어드밴스 [SEP] 24 32', '로날트 더 부르 [SEP] 21 28 [SEP] 네덜란드 [SEP] 31 34']\n",
      "0    1873년 스페인의 왕정이 폐지되고 공화국이 설립되자, 아마데오 1세는 퇴위당하고 ...\n",
      "1    인터넷은 1969 년 미국 국방부 부하의 고등 연구 계획국 (ARPA)에 의해 제안...\n",
      "2    손흥민의 토트넘은 15일 오후 11시 영국 울버햄튼의 몰리뉴 스타디움에서 울버햄튼 ...\n",
      "3    더불어 2017 시즌을 기점으로 K3리그가 K3리그 어드밴스로 개편되고, 하위 리그...\n",
      "4    또한 프랑크 더 부르의 쌍둥이 동생인 로날트 더 부르도 네덜란드 축구 국가대표팀과 ...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_word(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f'<{sub_type}>',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f'<{ob_type}>',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02, s01, s02, d01, d02 in zip(dataset['subject_entity'], dataset['object_entity'], dataset['sub_start'], dataset['ob_start'], dataset['sub_end'], dataset['ob_end']):\n",
    "    temp = ''\n",
    "    temp = e01 + ' [SEP] '+ f'{s01} {d01}' + ' [SEP] ' + e02+ ' [SEP] '+ f'{s02} {d02}'\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #                                                                 dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "  \n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "                                  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_totalX3.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
