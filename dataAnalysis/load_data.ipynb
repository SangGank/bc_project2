{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing_dataset(dataset):\n",
    "  \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
    "  subject_entity = []\n",
    "  object_entity = []\n",
    "  subject_type= []\n",
    "  object_type= []\n",
    "  subject_start= []\n",
    "  object_start= []\n",
    "  subject_end= []\n",
    "  object_end= []\n",
    "  \n",
    "\n",
    "  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    # i = i[1:-1].split(',')[0].split(':')[1]\n",
    "    sub_entity = eval(i)\n",
    "    ob_entity = eval(j)\n",
    "    sub_word= sub_entity['word']\n",
    "    sub_type = sub_entity['type']\n",
    "    # j = j[1:-1].split(',')[0].split(':')[1]\n",
    "    ob_word = ob_entity['word']\n",
    "    ob_type = ob_entity['type']\n",
    "\n",
    "    ob_start = ob_entity['start_idx']\n",
    "    sub_start = sub_entity['start_idx']\n",
    "\n",
    "    ob_end = ob_entity['end_idx']\n",
    "    sub_end = sub_entity['end_idx']\n",
    "\n",
    "\n",
    "    subject_entity.append(sub_word)\n",
    "    object_entity.append(ob_word)\n",
    "    subject_type.append(sub_type)\n",
    "    object_type.append(ob_type)\n",
    "    subject_start.append(sub_start)\n",
    "    object_start.append(ob_start)\n",
    "    subject_end.append(sub_end)\n",
    "    object_end.append(ob_end)\n",
    "  \n",
    "  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],\n",
    "                              'subject_type' : subject_type, 'object_type': object_type,'sub_start' : subject_start,'ob_start':object_start,'sub_end' :subject_end ,\n",
    "                              'ob_end': object_end,'source':dataset['source']})\n",
    "  out_dataset.id = out_dataset.index\n",
    "  return out_dataset\n",
    "\n",
    "\n",
    "  \n",
    "def load_data(dataset_dir):\n",
    "  \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
    "  pd_dataset = pd.read_csv(dataset_dir)\n",
    "  dataset = preprocessing_dataset(pd_dataset)\n",
    "  \n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 기본 베이스 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Runch Randa [SEP] 방탄소년단', '위키백과 [SEP] 위키미디어 재단', '임재범 [SEP] 송남영', '산청군 [SEP] 이재근', '유신정우회 [SEP] 한국국민당']\n",
      "0    방탄소년단 멤버로 데뷔하기 전에 그는 《Runch Randa》라는 이름인 언더 래퍼였다.\n",
      "1    위키백과를 운영하고 있는 위키미디어 재단은 매년 세계적인 컨퍼런스인 위키마니아 행사...\n",
      "2    그는 이화여대 미대 출신의 한국 여성 정재은씨와 결혼해 두 딸을 두고 있고, 임재범...\n",
      "3    경남 산청군 산청군청에서 열린 협약식에는 최재호 무학그룹 회장과 이재근 산청군수 외...\n",
      "4    1980년 구 민주공화당 인사들과 유신정우회 인사들이 한국국민당을 창당하였고, 19...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = e01 + ' [SEP] ' + e02\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_totalX3.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 단어를 토큰으로 바꾸기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Runch Randa [SEP] 방탄소년단', '위키백과 [SEP] 위키미디어 재단', '임재범 [SEP] 송남영', '산청군 [SEP] 이재근', '유신정우회 [SEP] 한국국민당']\n",
      "0          <PER> 멤버로 데뷔하기 전에 그는 《<PER>》라는 이름인 언더 래퍼였다.\n",
      "1    <ORG>를 운영하고 있는 <ORG>은 매년 세계적인 컨퍼런스인 위키마니아 행사를 ...\n",
      "2    그는 이화여대 미대 출신의 한국 여성 정재은씨와 결혼해 두 딸을 두고 있고, <PE...\n",
      "3    경남 <ORG> <ORG>청에서 열린 협약식에는 최재호 무학그룹 회장과 <PER> ...\n",
      "4    1980년 구 민주공화당 인사들과 <ORG> 인사들이 <ORG>을 창당하였고, 19...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_word(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f'<{sub_type}>',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f'<{ob_type}>',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = e01 + ' [SEP] ' + e02\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "  \n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "                                  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_totalX3.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 인덱스 스타트 번호 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Runch Randa [SEP] 22 [SEP] 방탄소년단 [SEP] 0', '위키백과 [SEP] 0 [SEP] 위키미디어 재단 [SEP] 14', '임재범 [SEP] 43 [SEP] 송남영 [SEP] 52', '산청군 [SEP] 7 [SEP] 이재근 [SEP] 36', '유신정우회 [SEP] 19 [SEP] 한국국민당 [SEP] 30']\n",
      "0    방탄소년단 멤버로 데뷔하기 전에 그는 《Runch Randa》라는 이름인 언더 래퍼였다.\n",
      "1    위키백과를 운영하고 있는 위키미디어 재단은 매년 세계적인 컨퍼런스인 위키마니아 행사...\n",
      "2    그는 이화여대 미대 출신의 한국 여성 정재은씨와 결혼해 두 딸을 두고 있고, 임재범...\n",
      "3    경남 산청군 산청군청에서 열린 협약식에는 최재호 무학그룹 회장과 이재근 산청군수 외...\n",
      "4    1980년 구 민주공화당 인사들과 유신정우회 인사들이 한국국민당을 창당하였고, 19...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_word(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f'<{sub_type}>',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f'<{ob_type}>',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02, s01, s02 in zip(dataset['subject_entity'], dataset['object_entity'], dataset['sub_start'], dataset['ob_start']):\n",
    "    temp = ''\n",
    "    temp = e01 + ' [SEP] '+ f'{s01}' + ' [SEP] ' + e02+ ' [SEP] '+ f'{s02}'\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #                                                                 dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "  \n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "                                  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_totalX3.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 스타트 번호, 끝번호 둘다 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Runch Randa [SEP] 22 32 [SEP] 방탄소년단 [SEP] 0 4', '위키백과 [SEP] 0 3 [SEP] 위키미디어 재단 [SEP] 14 21', '임재범 [SEP] 43 45 [SEP] 송남영 [SEP] 52 54', '산청군 [SEP] 7 9 [SEP] 이재근 [SEP] 36 38', '유신정우회 [SEP] 19 23 [SEP] 한국국민당 [SEP] 30 34']\n",
      "0    방탄소년단 멤버로 데뷔하기 전에 그는 《Runch Randa》라는 이름인 언더 래퍼였다.\n",
      "1    위키백과를 운영하고 있는 위키미디어 재단은 매년 세계적인 컨퍼런스인 위키마니아 행사...\n",
      "2    그는 이화여대 미대 출신의 한국 여성 정재은씨와 결혼해 두 딸을 두고 있고, 임재범...\n",
      "3    경남 산청군 산청군청에서 열린 협약식에는 최재호 무학그룹 회장과 이재근 산청군수 외...\n",
      "4    1980년 구 민주공화당 인사들과 유신정우회 인사들이 한국국민당을 창당하였고, 19...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_word(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f'<{sub_type}>',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f'<{ob_type}>',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02, s01, s02, d01, d02 in zip(dataset['subject_entity'], dataset['object_entity'], dataset['sub_start'], dataset['ob_start'], dataset['sub_end'], dataset['ob_end']):\n",
    "    temp = ''\n",
    "    temp = e01 + ' [SEP] '+ f'{s01} {d01}' + ' [SEP] ' + e02+ ' [SEP] '+ f'{s02} {d02}'\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #                                                                 dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "  \n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "                                  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_totalX3.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. object이랑 subject 각각 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<ob> Runch Randa [SEP] <sub> 방탄소년단', '<ob> 위키백과 [SEP] <sub> 위키미디어 재단', '<ob> 임재범 [SEP] <sub> 송남영', '<ob> 산청군 [SEP] <sub> 이재근', '<ob> 유신정우회 [SEP] <sub> 한국국민당']\n",
      "0    방탄소년단 멤버로 데뷔하기 전에 그는 《Runch Randa》라는 이름인 언더 래퍼였다.\n",
      "1    위키백과를 운영하고 있는 위키미디어 재단은 매년 세계적인 컨퍼런스인 위키마니아 행사...\n",
      "2    그는 이화여대 미대 출신의 한국 여성 정재은씨와 결혼해 두 딸을 두고 있고, 임재범...\n",
      "3    경남 산청군 산청군청에서 열린 협약식에는 최재호 무학그룹 회장과 이재근 산청군수 외...\n",
      "4    1980년 구 민주공화당 인사들과 유신정우회 인사들이 한국국민당을 창당하였고, 19...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = '<ob> ' + e01 + ' [SEP] ' + '<sub> ' + e02\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_totalX3.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 타입별로 모델을 다르게 만드는것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 한문장에 다 때려박기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    〈Something〉는  <o> <PER> </o>이 쓰고  <s> <ORG> </...\n",
      "1    호남이 기반인 바른미래당· <o> <ORG> </o>· <s> <ORG> </s>이...\n",
      "2    K리그2에서 성적 1위를 달리고 있는  <s> <ORG> </s>는 지난 26일  ...\n",
      "3    균일가 생활용품점 (주) <s> <ORG> </s>(대표  <o> <PER> </o...\n",
      "4     <o> <DAT> </o>년 프로 야구 드래프트 1순위로  <s> <ORG> </...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_sentence(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f' <s> <{sub_type}> </s>',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f' <o> <{ob_type}> </o>',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_sentence(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "  \n",
    "  # print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "                                  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 토큰을 object,subject 구분하기 - 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    〈Something〉는 <PER>이 쓰고 <ORG>가 1969년 앨범 《Abbey ...\n",
      "1    호남이 기반인 바른미래당·<ORG>·<ORG>이 우여곡절 끝에 합당해 민생당(가칭)...\n",
      "2    K리그2에서 성적 1위를 달리고 있는 <ORG>는 지난 26일 <ORG>으로부터 관...\n",
      "3    균일가 생활용품점 (주)<ORG>(대표 <PER>)는 코로나19 바이러스로 어려움을...\n",
      "4    <DAT>년 프로 야구 드래프트 1순위로 <ORG>에게 입단하면서 등번호는 8번으로...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_word2(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f' <S. {sub_type}> ',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f' <O. {ob_type}> ',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  \n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "  \n",
    "#   print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "                                  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. loss function focal로 바꿔보기 - 14번이랑 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "\n",
    "#         # calculate focal loss\n",
    "#         alpha = [0.139, 0.179, 0.212, 0.221, 0.815, 1.891, 6.016, 8.694]\n",
    "#         alpha = torch.tensor(alpha).to(self.args.device)\n",
    "#         gamma = 2.0\n",
    "\n",
    "#         ce_loss = torch.nn.functional.cross_entropy(logits, labels, reduction='none')\n",
    "#         pt = torch.exp(-ce_loss)\n",
    "#         focal_loss = alpha[labels] * (1-pt)**gamma * ce_loss\n",
    "\n",
    "#         loss = torch.mean(focal_loss)\n",
    "\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 이름도 넣고 라벨도 넣는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    〈Something〉는  <o> 조지 해리슨 <PER> </o> 이 쓰고  <s> ...\n",
      "1    호남이 기반인 바른미래당· <o> 대안신당 <ORG> </o> · <s> 민주평화당...\n",
      "2    K리그2에서 성적 1위를 달리고 있는  <s> 광주FC <ORG> </s> 는 지난...\n",
      "3    균일가 생활용품점 (주) <s> 아성다이소 <ORG> </s> (대표  <o> 박정...\n",
      "4     <o> 1967 <DAT> </o> 년 프로 야구 드래프트 1순위로  <s> 요미...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_sentence2(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f' <s> {sub_word} <{sub_type}> </s> ',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f' <o> {ob_word} <{ob_type}> </o> ',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_sentence2(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "  \n",
    "  # print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "                                  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 2번째 문장에 10번 내용 넣기 \n",
    "##### 기존 sentence 그대로 두고 2번째 문장에 태그와 타입 이름 모두 넣는다 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> 비틀즈 <ORG> </s> [SEP] <o> 조지 해리슨 <PER> </o> ', '<s> 민주평화당 <ORG> </s> [SEP] <o> 대안신당 <ORG> </o> ', '<s> 광주FC <ORG> </s> [SEP] <o> 한국프로축구연맹 <ORG> </o> ', '<s> 아성다이소 <ORG> </s> [SEP] <o> 박정부 <PER> </o> ', '<s> 요미우리 자이언츠 <ORG> </s> [SEP] <o> 1967 <DAT> </o> ']\n",
      "0    〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...\n",
      "1    호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...\n",
      "2    K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...\n",
      "3    균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...\n",
      "4    1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02, t01, t02 in zip(dataset['subject_entity'], dataset['object_entity'],dataset['subject_type'] ,dataset['object_type']):\n",
    "    temp = ''\n",
    "    temp =  f'<s> {e01} <{t01}> </s>' + ' [SEP] ' + f'<o> {e02} <{t02}> </o> '\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  \n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. 11번에 8번 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Runch Randa <S. PER> </s> [SEP] <o> 방탄소년단 <O. PER> </o> ', '<s> 위키백과 <S. ORG> </s> [SEP] <o> 위키미디어 재단 <O. ORG> </o> ', '<s> 임재범 <S. PER> </s> [SEP] <o> 송남영 <O. PER> </o> ', '<s> 산청군 <S. ORG> </s> [SEP] <o> 이재근 <O. PER> </o> ', '<s> 유신정우회 <S. ORG> </s> [SEP] <o> 한국국민당 <O. ORG> </o> ']\n",
      "0          <PER> 멤버로 데뷔하기 전에 그는 《<PER>》라는 이름인 언더 래퍼였다.\n",
      "1    <ORG>를 운영하고 있는 <ORG>은 매년 세계적인 컨퍼런스인 위키마니아 행사를 ...\n",
      "2    그는 이화여대 미대 출신의 한국 여성 정재은씨와 결혼해 두 딸을 두고 있고, <PE...\n",
      "3    경남 <ORG> <ORG>청에서 열린 협약식에는 최재호 무학그룹 회장과 <PER> ...\n",
      "4    1980년 구 민주공화당 인사들과 <ORG> 인사들이 <ORG>을 창당하였고, 19...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02, t01, t02 in zip(dataset['subject_entity'], dataset['object_entity'],dataset['subject_type'] ,dataset['object_type']):\n",
    "    temp = ''\n",
    "    temp =  f'<s> {e01} <S. {t01}> </s>' + ' [SEP] ' + f'<o> {e02} <O. {t02}> </o> '\n",
    "    concat_entity.append(temp)\n",
    "    \n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "\n",
    "    \n",
    "  \n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_totalX3.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. 11번에 기존 문장에 타입이랑 태그를 추가(기존 단어는 그대로 냅둠, 11번이랑 10번이랑 합친거와 같음)  - 현재 베스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> 비틀즈 <ORG> </s> [SEP] <o> 조지 해리슨 <PER> </o> ', '<s> 민주평화당 <ORG> </s> [SEP] <o> 대안신당 <ORG> </o> ', '<s> 광주FC <ORG> </s> [SEP] <o> 한국프로축구연맹 <ORG> </o> ', '<s> 아성다이소 <ORG> </s> [SEP] <o> 박정부 <PER> </o> ', '<s> 요미우리 자이언츠 <ORG> </s> [SEP] <o> 1967 <DAT> </o> ']\n",
      "0    〈Something〉는  <o> 조지 해리슨 <PER> </o> 이 쓰고  <s> ...\n",
      "1    호남이 기반인 바른미래당· <o> 대안신당 <ORG> </o> · <s> 민주평화당...\n",
      "2    K리그2에서 성적 1위를 달리고 있는  <s> 광주FC <ORG> </s> 는 지난...\n",
      "3    균일가 생활용품점 (주) <s> 아성다이소 <ORG> </s> (대표  <o> 박정...\n",
      "4     <o> 1967 <DAT> </o> 년 프로 야구 드래프트 1순위로  <s> 요미...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02, t01, t02 in zip(dataset['subject_entity'], dataset['object_entity'],dataset['subject_type'] ,dataset['object_type']):\n",
    "    temp = ''\n",
    "    temp =  f'<s> {e01} <{t01}> </s>' + ' [SEP] ' + f'<o> {e02} <{t02}> </o> '\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_sentence2(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "\n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. 13번에서 <PER 말고 <S. PER> 과 같이 바꾸기 - 13번보다 높음  ****현재 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> 비틀즈 <S. ORG> </s> [SEP] <o> 조지 해리슨 <O. PER> </o> ', '<s> 민주평화당 <S. ORG> </s> [SEP] <o> 대안신당 <O. ORG> </o> ', '<s> 광주FC <S. ORG> </s> [SEP] <o> 한국프로축구연맹 <O. ORG> </o> ', '<s> 아성다이소 <S. ORG> </s> [SEP] <o> 박정부 <O. PER> </o> ', '<s> 요미우리 자이언츠 <S. ORG> </s> [SEP] <o> 1967 <O. DAT> </o> ']\n",
      "0    〈Something〉는  <o> 조지 해리슨 <O. PER> </o> 이 쓰고  <...\n",
      "1    호남이 기반인 바른미래당· <o> 대안신당 <O. ORG> </o> · <s> 민주...\n",
      "2    K리그2에서 성적 1위를 달리고 있는  <s> 광주FC <S. ORG> </s> 는...\n",
      "3    균일가 생활용품점 (주) <s> 아성다이소 <S. ORG> </s> (대표  <o>...\n",
      "4     <o> 1967 <O. DAT> </o> 년 프로 야구 드래프트 1순위로  <s>...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_sentence3(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f' <s> {sub_word} <S. {sub_type}> </s> ',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f' <o> {ob_word} <O. {ob_type}> </o> ',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02, t01, t02 in zip(dataset['subject_entity'], dataset['object_entity'],dataset['subject_type'] ,dataset['object_type']):\n",
    "    temp = ''\n",
    "    temp =  f'<s> {e01} <S. {t01}> </s>' + ' [SEP] ' + f'<o> {e02} <O. {t02}> </o> '\n",
    "\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_sentence3(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "\n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. 14번 뒤에 source 붙이기 - 조금 더 상승"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wikipedia' 'wikitree' 'policy_briefing']\n",
      "['<s> 비틀즈 <S. ORG> </s> [SEP] <o> 조지 해리슨 <O. PER> </o> [SEP] <wikipedia>', '<s> 민주평화당 <S. ORG> </s> [SEP] <o> 대안신당 <O. ORG> </o> [SEP] <wikitree>', '<s> 광주FC <S. ORG> </s> [SEP] <o> 한국프로축구연맹 <O. ORG> </o> [SEP] <wikitree>', '<s> 아성다이소 <S. ORG> </s> [SEP] <o> 박정부 <O. PER> </o> [SEP] <wikitree>', '<s> 요미우리 자이언츠 <S. ORG> </s> [SEP] <o> 1967 <O. DAT> </o> [SEP] <wikipedia>']\n",
      "0    〈Something〉는  <o> 조지 해리슨 <O. PER> </o> 이 쓰고  <...\n",
      "1    호남이 기반인 바른미래당· <o> 대안신당 <O. ORG> </o> · <s> 민주...\n",
      "2    K리그2에서 성적 1위를 달리고 있는  <s> 광주FC <S. ORG> </s> 는...\n",
      "3    균일가 생활용품점 (주) <s> 아성다이소 <S. ORG> </s> (대표  <o>...\n",
      "4     <o> 1967 <O. DAT> </o> 년 프로 야구 드래프트 1순위로  <s>...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "\n",
    "  concat_entity = []\n",
    "  for e01, e02, t01, t02, source in zip(dataset['subject_entity'], dataset['object_entity'],dataset['subject_type'] ,dataset['object_type'], dataset['source']):\n",
    "    temp = ''\n",
    "    temp =  f'<s> {e01} <S. {t01}> </s>' + ' [SEP] ' + f'<o> {e02} <O. {t02}> </o> [SEP] <{source}>'\n",
    "\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_sentence3(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "\n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "print(train_dataset.source.unique())\n",
    "tokenized_train = tokenized_dataset(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16 : @ * person * Bill과 # ^ city ^ Seattle 사이의 관계는 무엇인가? 형식으로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@ * <S. ORG> * 비틀즈과 # ^ <O. PER> ^ 조지 해리슨 사이의 관계는 무엇인가?', '@ * <S. ORG> * 민주평화당과 # ^ <O. ORG> ^ 대안신당 사이의 관계는 무엇인가?', '@ * <S. ORG> * 광주FC과 # ^ <O. ORG> ^ 한국프로축구연맹 사이의 관계는 무엇인가?', '@ * <S. ORG> * 아성다이소과 # ^ <O. PER> ^ 박정부 사이의 관계는 무엇인가?', '@ * <S. ORG> * 요미우리 자이언츠과 # ^ <O. DAT> ^ 1967 사이의 관계는 무엇인가?']\n",
      "0    〈Something〉는  <o> 조지 해리슨 <O. PER> </o> 이 쓰고  <...\n",
      "1    호남이 기반인 바른미래당· <o> 대안신당 <O. ORG> </o> · <s> 민주...\n",
      "2    K리그2에서 성적 1위를 달리고 있는  <s> 광주FC <S. ORG> </s> 는...\n",
      "3    균일가 생활용품점 (주) <s> 아성다이소 <S. ORG> </s> (대표  <o>...\n",
      "4     <o> 1967 <O. DAT> </o> 년 프로 야구 드래프트 1순위로  <s>...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02, t01, t02 in zip(dataset['subject_entity'], dataset['object_entity'],dataset['subject_type'] ,dataset['object_type']):\n",
    "    temp = ''\n",
    "    temp =  f'@ * <S. {t01}> * {e01}과 # ^ <O. {t02}> ^ {e02} 사이의 관계는 무엇인가?'\n",
    "\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_sentence3(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "\n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. 16번에서 dataset sentence 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@ * <S. ORG> * 비틀즈과 # ^ <O. PER> ^ 조지 해리슨 사이의 관계는 무엇인가?', '@ * <S. ORG> * 민주평화당과 # ^ <O. ORG> ^ 대안신당 사이의 관계는 무엇인가?', '@ * <S. ORG> * 광주FC과 # ^ <O. ORG> ^ 한국프로축구연맹 사이의 관계는 무엇인가?', '@ * <S. ORG> * 아성다이소과 # ^ <O. PER> ^ 박정부 사이의 관계는 무엇인가?', '@ * <S. ORG> * 요미우리 자이언츠과 # ^ <O. DAT> ^ 1967 사이의 관계는 무엇인가?']\n",
      "0    〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...\n",
      "1    호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...\n",
      "2    K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...\n",
      "3    균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...\n",
      "4    1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02, t01, t02 in zip(dataset['subject_entity'], dataset['object_entity'],dataset['subject_type'] ,dataset['object_type']):\n",
    "    temp = ''\n",
    "    temp =  f'@ * <S. {t01}> * {e01}과 # ^ <O. {t02}> ^ {e02} 사이의 관계는 무엇인가?'\n",
    "\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "\n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. 16번에서 dataset sentence 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@ * <S. ORG> * 비틀즈과 # ^ <O. PER> ^ 조지 해리슨 사이의 관계는 무엇인가?', '@ * <S. ORG> * 민주평화당과 # ^ <O. ORG> ^ 대안신당 사이의 관계는 무엇인가?', '@ * <S. ORG> * 광주FC과 # ^ <O. ORG> ^ 한국프로축구연맹 사이의 관계는 무엇인가?', '@ * <S. ORG> * 아성다이소과 # ^ <O. PER> ^ 박정부 사이의 관계는 무엇인가?', '@ * <S. ORG> * 요미우리 자이언츠과 # ^ <O. DAT> ^ 1967 사이의 관계는 무엇인가?']\n",
      "0    〈Something〉는  # ^ <O. PER> ^ 조지 해리슨 이 쓰고  @ * ...\n",
      "1    호남이 기반인 바른미래당· # ^ <O. ORG> ^ 대안신당 · @ * <S. O...\n",
      "2    K리그2에서 성적 1위를 달리고 있는  @ * <S. ORG> * 광주FC 는 지난...\n",
      "3    균일가 생활용품점 (주) @ * <S. ORG> * 아성다이소 (대표  # ^ <O...\n",
      "4     # ^ <O. DAT> ^ 1967 년 프로 야구 드래프트 1순위로  @ * <S...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_sentence4(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f' @ * <S. {sub_type}> * {sub_word} ',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f' # ^ <O. {ob_type}> ^ {ob_word} ',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02, t01, t02 in zip(dataset['subject_entity'], dataset['object_entity'],dataset['subject_type'] ,dataset['object_type']):\n",
    "    temp = ''\n",
    "    temp =  f'@ * <S. {t01}> * {e01}과 # ^ <O. {t02}> ^ {e02} 사이의 관계는 무엇인가?'\n",
    "\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_sentence4(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "\n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. dataset sentence 변경하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['비틀즈와 조지 해리슨의 관계는 조직와 사람의 관계이다.', '민주평화당와 대안신당의 관계는 조직와 조직의 관계이다.', '광주FC와 한국프로축구연맹의 관계는 조직와 조직의 관계이다.', '아성다이소와 박정부의 관계는 조직와 사람의 관계이다.', '요미우리 자이언츠와 1967의 관계는 조직와 날짜의 관계이다.']\n",
      "0    〈Something〉는  # ^ <O. PER> ^ 조지 해리슨 이 쓰고  @ * ...\n",
      "1    호남이 기반인 바른미래당· # ^ <O. ORG> ^ 대안신당 · @ * <S. O...\n",
      "2    K리그2에서 성적 1위를 달리고 있는  @ * <S. ORG> * 광주FC 는 지난...\n",
      "3    균일가 생활용품점 (주) @ * <S. ORG> * 아성다이소 (대표  # ^ <O...\n",
      "4     # ^ <O. DAT> ^ 1967 년 프로 야구 드래프트 1순위로  @ * <S...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_sentence4(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f' @ * <S. {sub_type}> * {sub_word} ',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f' # ^ <O. {ob_type}> ^ {ob_word} ',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  change_type = {'PER' : '사람', 'ORG': '조직', 'DAT':'날짜', 'LOC': '지역', 'POH' : '대체어', 'NOH' : '숫자'}\n",
    "  concat_entity = []\n",
    "  for e01, e02, t01, t02 in zip(dataset['subject_entity'], dataset['object_entity'],dataset['subject_type'] ,dataset['object_type']):\n",
    "    temp = ''\n",
    "    temp =  f'{e01}와 {e02}의 관계는 {change_type[t01]}와 {change_type[t02]}의 관계이다.'\n",
    "\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_sentence4(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "\n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. sentence도 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['비틀즈와 조지 해리슨의 관계는 조직와 사람의 관계이다.', '민주평화당와 대안신당의 관계는 조직와 조직의 관계이다.', '광주FC와 한국프로축구연맹의 관계는 조직와 조직의 관계이다.', '아성다이소와 박정부의 관계는 조직와 사람의 관계이다.', '요미우리 자이언츠와 1967의 관계는 조직와 날짜의 관계이다.']\n",
      "0    〈Something〉는  조지 해리슨(사람)이 쓰고  비틀즈(조직)가 1969년 앨...\n",
      "1    호남이 기반인 바른미래당· 대안신당(조직)· 민주평화당(조직)이 우여곡절 끝에 합당...\n",
      "2    K리그2에서 성적 1위를 달리고 있는  광주FC(조직)는 지난 26일  한국프로축구...\n",
      "3    균일가 생활용품점 (주) 아성다이소(조직)(대표  박정부(사람))는 코로나19 바이...\n",
      "4     1967(날짜)년 프로 야구 드래프트 1순위로  요미우리 자이언츠(조직)에게 입단...\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def change_sentence5(sentence, sub_word, sub_type , ob_word, ob_type,change_type ):\n",
    "  sentence = re.sub(rf'{sub_word}',f' {sub_word}({change_type[sub_type]})',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f' {ob_word}({change_type[ob_type]})',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset(dataset):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  change_type = {'PER' : '사람', 'ORG': '조직', 'DAT':'날짜', 'LOC': '지역', 'POH' : '대체어', 'NOH' : '숫자'}\n",
    "  concat_entity = []\n",
    "  for e01, e02, t01, t02 in zip(dataset['subject_entity'], dataset['object_entity'],dataset['subject_type'] ,dataset['object_type']):\n",
    "    temp = ''\n",
    "    temp =  f'{e01}와 {e02}의 관계는 {change_type[t01]}와 {change_type[t02]}의 관계이다.'\n",
    "\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_sentence5(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x],change_type))\n",
    "\n",
    "  print(concat_entity[:5])\n",
    "  print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. 뒤에 번호 적기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def change_sentence4(sentence, sub_word, sub_type , ob_word, ob_type):\n",
    "  sentence = re.sub(rf'{sub_word}',f' @ * <S. {sub_type}> * {sub_word} ',sentence)\n",
    "  sentence = re.sub(rf'{ob_word}',f' # ^ <O. {ob_type}> ^ {ob_word} ',sentence)\n",
    "\n",
    "  return sentence\n",
    "\n",
    "def tokenized_dataset21(dataset,tokenizer):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  change_type = {'PER' : '사람', 'ORG': '조직', 'DAT':'날짜', 'LOC': '지역', 'POH' : '대체어', 'NOH' : '숫자'}\n",
    "  concat_entity = []\n",
    "  for e01, e02, t01, t02 in zip(dataset['subject_entity'], dataset['object_entity'],dataset['subject_type'] ,dataset['object_type']):\n",
    "    temp =  f'{e01}와 {e02}의 관계는 {change_type[t01]}와 {change_type[t02]}의 관계이다. (두번째 단어가 조직이라면 결과는 0, 1, 2, 3, 5, 7, 9, 18, 19, 20, 22, 28 중 하나이다.)'\n",
    "\n",
    "    concat_entity.append(temp)\n",
    "  \n",
    "  dataset['sentence'] = dataset['id'].apply(lambda x: change_sentence4(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "                                                                  dataset.subject_type.loc[x],dataset.object_entity.loc[x], dataset.object_type.loc[x]))\n",
    "  \n",
    "  tokenized_sentences = tokenizer(\n",
    "      concat_entity,\n",
    "      list(dataset['sentence']),\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      max_length=512,\n",
    "      add_special_tokens=True,\n",
    "      )\n",
    "  return tokenized_sentences\n",
    "\n",
    "# print(concat_entity[:5])\n",
    "# print(dataset['sentence'].head())\n",
    "  # dataset['sentence'] = dataset['id'].apply(lambda x: change_word(dataset.sentence.loc[x],dataset.subject_entity.loc[x],\n",
    "  #  \n",
    "train_dataset = load_data(\"../data/dataset/train/train_type.csv\")\n",
    "tokenized_train = tokenized_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no_relation': 0, 'org:top_members/employees': 1, 'org:members': 2, 'org:product': 3, 'per:title': 4, 'org:alternate_names': 5, 'per:employee_of': 6, 'org:place_of_headquarters': 7, 'per:product': 8, 'org:number_of_employees/members': 9, 'per:children': 10, 'per:place_of_residence': 11, 'per:alternate_names': 12, 'per:other_family': 13, 'per:colleagues': 14, 'per:origin': 15, 'per:siblings': 16, 'per:spouse': 17, 'org:founded': 18, 'org:political/religious_affiliation': 19, 'org:member_of': 20, 'per:parents': 21, 'org:dissolved': 22, 'per:schools_attended': 23, 'per:date_of_death': 24, 'per:date_of_birth': 25, 'per:place_of_birth': 26, 'per:place_of_death': 27, 'org:founded_by': 28, 'per:religion': 29}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('../code/dict_label_to_num.pkl', 'rb') as f:\n",
    "  dict_label_to_num = pickle.load(f)\n",
    "  print(dict_label_to_num)\n",
    "\n",
    "0, 1, 2, 3, 5, 7, 9, 18, 19, 20, 22, 28"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
